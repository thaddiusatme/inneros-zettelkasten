---
type: permanent
created: '2025-07-25'
modified: '2025-07-25'
status: inbox
tags: [whisper, llm, prompt-engineering, ai]
visibility: private
---
# Whisper + LLM Prompt Engineering

## Core Idea
Combining OpenAI's Whisper (speech-to-text) with LLM prompt engineering creates powerful voice-first workflows that maintain context and intent across modalities.

## Why It Matters
Voice input is often more natural and faster than typing, but converting voice to actionable tasks typically loses nuance and context. By engineering prompts specifically for voice-transcribed content, we can preserve intent and extract structured data more effectively.

## Key Points
- Whisper's accuracy benefits from prompt templates that account for transcription quirks
- Voice input requires different prompt structures than text input
- Context preservation between voice → text → structured data requires specialized engineering
- Multi-step prompting (transcribe → structure → extract → format) yields better results than single-step

## Applications
- Voice memos to structured tasks
- Meeting recordings to action items
- Voice commands to workflow automation
- Field notes to documentation

## Links
- [[Voice-First Ops for Lean Teams]]
- [[Async Task Clarity Through AI Voice Parsing]]
- [[Support SOP System Map]]

---

*Created on 2025-07-19*
