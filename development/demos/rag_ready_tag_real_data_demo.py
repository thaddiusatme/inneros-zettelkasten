#!/usr/bin/env python3
"""
RAG-Ready Tag Strategy Real Data Demo

Tests the RAG-Ready Tag Engine with actual vault data to demonstrate
tag cleanup, namespace classification, and rule generation capabilities.
"""

import sys
import os
import re
from pathlib import Path
from typing import List, Dict, Set, Any
import time

# Add development directory to path
current_dir = Path(__file__).parent
development_dir = current_dir.parent
sys.path.insert(0, str(development_dir))

from src.ai.rag_ready_tag_engine import RAGReadyTagEngine
from src.utils.frontmatter import parse_frontmatter
def scan_vault_for_real_tags(vault_path: Path) -> Dict[str, List[str]]:
    """Scan the actual vault for all tags and their sources"""
    print("🔍 Scanning vault for real tags...")
    
    tag_sources = {}
    total_files = 0
    files_with_tags = 0
    
    # Scan all markdown files
    for md_file in vault_path.rglob("*.md"):
        if ".obsidian" in str(md_file) or "web_ui_env" in str(md_file):
            continue
            
        total_files += 1
        try:
            content = md_file.read_text(encoding='utf-8')
            frontmatter, _ = parse_frontmatter(content)
            
            if frontmatter and 'tags' in frontmatter:
                tags = frontmatter['tags']
                if isinstance(tags, str):
                    tags = [tags]
                elif isinstance(tags, list):
                    # Flatten any nested structures
                    flat_tags = []
                    for tag in tags:
                        if isinstance(tag, str):
                            flat_tags.append(tag)
                        elif isinstance(tag, list):
                            flat_tags.extend([str(t) for t in tag])
                    tags = flat_tags
                
                if tags:
                    files_with_tags += 1
                    for tag in tags:
                        tag_str = str(tag).strip()
                        if tag_str:
                            if tag_str not in tag_sources:
                                tag_sources[tag_str] = []
                            tag_sources[tag_str].append(str(md_file.relative_to(vault_path)))
                            
        except Exception as e:
            print(f"⚠️  Error reading {md_file}: {e}")
            continue
    
    print(f"📊 Scan Results:")
    print(f"   • Total files scanned: {total_files}")
    print(f"   • Files with tags: {files_with_tags}")
    print(f"   • Unique tags found: {len(tag_sources)}")
    print()
    
    return tag_sources


def analyze_real_tags_with_engine(tag_sources: Dict[str, List[str]]) -> Dict[str, Any]:
    """Analyze real tags using our RAG-Ready Tag Engine"""
    print("🤖 Analyzing tags with RAG-Ready Tag Engine...")
    
    # Create engine instance
    engine = RAGReadyTagEngine("/tmp")  # Vault path not needed for analysis
    
    # Get all unique tags
    all_tags = list(tag_sources.keys())
    
    # Perform comprehensive analysis
    start_time = time.time()
    analysis = engine.cleanup_engine.analyze_all_tags(all_tags)
    analysis_time = time.time() - start_time
    
    # Add source information
    analysis['tag_sources'] = tag_sources
    analysis['analysis_time'] = analysis_time
    analysis['tags_per_second'] = len(all_tags) / analysis_time if analysis_time > 0 else 0
    
    return analysis


def display_problematic_tag_examples(analysis: Dict[str, Any]):
    """Display concrete examples of problematic tags found in real data"""
    print("🚨 PROBLEMATIC TAG EXAMPLES FOUND:")
    print("=" * 60)
    
    tag_sources = analysis['tag_sources']
    
    # Metadata redundant tags
    metadata_redundant = analysis.get('metadata_redundant', [])
    if metadata_redundant:
        print("\n📝 METADATA REDUNDANT TAGS:")
        print("   (Tags that duplicate YAML metadata fields)")
        for tag in metadata_redundant[:5]:  # Show first 5 examples
            sources = tag_sources[tag][:3]  # Show first 3 files
            print(f"   • '{tag}' found in {len(tag_sources[tag])} file(s)")
            for source in sources:
                print(f"     - {source}")
            if len(tag_sources[tag]) > 3:
                print(f"     - ... and {len(tag_sources[tag]) - 3} more files")
        if len(metadata_redundant) > 5:
            print(f"   ... and {len(metadata_redundant) - 5} more metadata redundant tags")
    
    # AI artifact tags
    ai_artifacts = analysis.get('ai_artifacts', [])
    if ai_artifacts:
        print("\n🤖 AI ARTIFACT TAGS:")
        print("   (Tags generated by AI that should be cleaned up)")
        for tag in ai_artifacts[:5]:
            sources = tag_sources[tag][:3]
            print(f"   • '{tag}' found in {len(tag_sources[tag])} file(s)")
            for source in sources:
                print(f"     - {source}")
            if len(tag_sources[tag]) > 3:
                print(f"     - ... and {len(tag_sources[tag]) - 3} more files")
        if len(ai_artifacts) > 5:
            print(f"   ... and {len(ai_artifacts) - 5} more AI artifact tags")
    
    # Parsing error tags
    parsing_errors = analysis.get('parsing_errors', [])
    if parsing_errors:
        print("\n❌ PARSING ERROR TAGS:")
        print("   (Malformed tags that need fixing)")
        for tag in parsing_errors[:5]:
            sources = tag_sources[tag][:3]
            print(f"   • '{tag}' (malformed) found in {len(tag_sources[tag])} file(s)")
            for source in sources:
                print(f"     - {source}")
            if len(tag_sources[tag]) > 3:
                print(f"     - ... and {len(tag_sources[tag]) - 3} more files")
        if len(parsing_errors) > 5:
            print(f"   ... and {len(parsing_errors) - 5} more parsing error tags")
    
    # Semantic duplicates
    semantic_duplicates = analysis.get('semantic_duplicates', [])
    if semantic_duplicates:
        print("\n🔄 SEMANTIC DUPLICATE GROUPS:")
        print("   (Tags with similar meanings that should be merged)")
        for i, group in enumerate(semantic_duplicates[:3]):  # Show first 3 groups
            print(f"   Group {i+1}: {group}")
            # Show usage for each tag in group
            for tag in group:
                if tag in tag_sources:
                    print(f"     • '{tag}' used in {len(tag_sources[tag])} file(s)")
        if len(semantic_duplicates) > 3:
            print(f"   ... and {len(semantic_duplicates) - 3} more semantic duplicate groups")


def demonstrate_namespace_classification(analysis: Dict[str, Any], engine: RAGReadyTagEngine):
    """Demonstrate namespace classification on real tags"""
    print("\n🏷️  NAMESPACE CLASSIFICATION EXAMPLES:")
    print("=" * 60)
    
    tag_sources = analysis['tag_sources']
    all_tags = list(tag_sources.keys())
    
    # Classify a sample of tags
    sample_tags = all_tags[:15] if len(all_tags) > 15 else all_tags
    classifications = engine.namespace_classifier.classify_batch(sample_tags)
    
    # Group by namespace
    by_namespace = {'type': [], 'topic': [], 'context': []}
    for tag, classification in zip(sample_tags, classifications):
        namespace = classification['namespace']
        canonical = classification['canonical_form']
        confidence = classification.get('confidence', 0.0)
        
        by_namespace[namespace].append({
            'original': tag,
            'canonical': canonical,
            'confidence': confidence,
            'usage_count': len(tag_sources[tag])
        })
    
    # Display results
    for namespace, tags in by_namespace.items():
        if tags:
            print(f"\n📂 {namespace.upper()} NAMESPACE:")
            for tag_info in tags:
                print(f"   • '{tag_info['original']}' → '{tag_info['canonical']}'")
                print(f"     Confidence: {tag_info['confidence']:.1f}, Used in {tag_info['usage_count']} file(s)")


def show_cleanup_recommendations(analysis: Dict[str, Any]):
    """Show concrete cleanup recommendations based on real data"""
    print("\n💡 CLEANUP RECOMMENDATIONS:")
    print("=" * 60)
    
    total_tags = analysis.get('total_tags', 0)
    problematic = analysis.get('total_problematic', 0)
    cleanup_percentage = analysis.get('cleanup_percentage', 0)
    
    print(f"\n📊 ANALYSIS SUMMARY:")
    print(f"   • Total tags analyzed: {total_tags}")
    print(f"   • Problematic tags found: {problematic}")
    print(f"   • Cleanup percentage: {cleanup_percentage:.1f}%")
    
    if 'performance_metrics' in analysis:
        perf = analysis['performance_metrics']
        print(f"   • Analysis time: {perf.get('analysis_time_seconds', 0):.3f} seconds")
        print(f"   • Processing speed: {perf.get('tags_per_second', 0):.0f} tags/second")
    
    print(f"\n🎯 RECOMMENDED ACTIONS:")
    
    metadata_redundant = len(analysis.get('metadata_redundant', []))
    ai_artifacts = len(analysis.get('ai_artifacts', []))
    parsing_errors = len(analysis.get('parsing_errors', []))
    semantic_duplicates = len(analysis.get('semantic_duplicates', []))
    
    if metadata_redundant > 0:
        print(f"   1. Remove {metadata_redundant} metadata redundant tags")
        print(f"      (These duplicate information already in YAML frontmatter)")
    
    if ai_artifacts > 0:
        print(f"   2. Clean up {ai_artifacts} AI artifact tags")
        print(f"      (These are AI-generated tags that add no semantic value)")
    
    if parsing_errors > 0:
        print(f"   3. Fix {parsing_errors} parsing error tags")
        print(f"      (These have formatting issues that break tag processing)")
    
    if semantic_duplicates > 0:
        print(f"   4. Merge {semantic_duplicates} semantic duplicate groups")
        print(f"      (These represent the same concepts with different names)")
    
    # Calculate potential impact
    if total_tags > 0:
        efficiency_gain = cleanup_percentage * 0.8  # Estimated efficiency improvement
        print(f"\n🚀 PROJECTED IMPACT:")
        print(f"   • Tag collection efficiency gain: ~{efficiency_gain:.1f}%")
        print(f"   • Reduced cognitive load for tag selection")
        print(f"   • Improved RAG system semantic accuracy")
        print(f"   • Enhanced AI connection discovery performance")


def main():
    """Main demo function"""
    print("🏷️  RAG-Ready Tag Strategy Real Data Demo")
    print("=" * 60)
    
    # Get vault root - use the repo root
    vault_root = Path(__file__).parent.parent.parent  # Go up from demos/development/repo
    knowledge_dir = vault_root / "knowledge"
    
    if knowledge_dir.exists():
        vault_path = knowledge_dir
        print(f"📁 Analyzing vault knowledge directory: {vault_path}")
    else:
        vault_path = vault_root
        print(f"📁 Analyzing full repo: {vault_path}")
    
    print()
    
    # Scan for real tags
    tag_sources = scan_vault_for_real_tags(vault_path)
    
    if not tag_sources:
        print("❌ No tags found in vault!")
        return
    
    # Analyze with our engine
    analysis = analyze_real_tags_with_engine(tag_sources)
    
    # Create engine for additional demos
    engine = RAGReadyTagEngine(str(vault_root))
    
    # Display results
    display_problematic_tag_examples(analysis)
    demonstrate_namespace_classification(analysis, engine)
    show_cleanup_recommendations(analysis)
    
    print(f"\n✅ Real data analysis complete!")
    print(f"📈 Found concrete examples of tag cleanup opportunities in your vault")
    print(f"🚀 RAG-Ready Tag Strategy Engine validated with live data")


if __name__ == "__main__":
    main()
